{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib3\n",
    "import certifi\n",
    "import json\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct all regular expressions\n",
    "title_regex = re.compile(r\"class=\\\"title\\\"><h3>(.+)</h3>\")\n",
    "date_regex = re.compile(r\"class=\\\"clamp-details\\\">\\s+<span>(.+)</span>\")\n",
    "description_regex = re.compile(r\"class=\\\"summary\\\">\\s*([\\S\\s]+?)\\s*<\\/div>\")\n",
    "score_regex = re.compile(r\"<span class=\\\"title\\\">Metascore:<\\/span>\\s+<a class=\\\"metascore_anchor\\\" href=\\\"\\/movie\\/.*?\\/critic-reviews\\\">\\s+<div class=\\\"metascore_w large movie.+\\\">(.*?)<\\/div>\")\n",
    "image_regex = re.compile(r\"<a href =\\\"/movie/.*\\\"><img src=\\\"(.*)\\\" alt=\\\"\")\n",
    "\n",
    "http = urllib3.PoolManager(ca_certs=certifi.where())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve credentials\n",
    "with open(\"/Users/tiffanivick/Desktop/credentials.json\") as f:\n",
    "  data = json.load(f)\n",
    "  mongo_connection_string = data ['mongodb']\n",
    "  \n",
    "# Fetch the database named \"DB1\"\n",
    "client = pymongo.MongoClient(mongo_connection_string, tlsCAFile=certifi.where())\n",
    "db1_database = client['DB1']\n",
    "metacritic_data = db1_database['metacritic-movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a list of movies from a particular year and page of Metacritic\n",
    "def metacritic_scraper(year: int, page: int) -> pd.DataFrame:\n",
    "  \n",
    "  # Fetch the webpage\n",
    "  url = \"https://www.metacritic.com/browse/movies/score/metascore/year/filtered?year_selected=(year)&sort=desc&view=detailed&page=(page)\"\n",
    "  response = http.request('GET', url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "  datastring = str(response.data, 'utf-8')\n",
    "  \n",
    "  # Execute all the regular expressions\n",
    "  titles = title_regex.findall(datastring)\n",
    "  dates = date_regex.findall(datastring)\n",
    "  descriptions = description_regex.findall(datastring)\n",
    "  scores = score_regex.findall(datastring)\n",
    "  images = image_regex.findall(datastring)\n",
    "  \n",
    "  # Return a unified collection\n",
    "  dataset = {'title': titles, 'dates': dates, 'descriptions': descriptions, 'scores': scores, 'images': images}\n",
    "  \n",
    "  return pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for 2000 page 0...\n",
      "Collecting data for 2001 page 0...\n",
      "Collecting data for 2002 page 0...\n",
      "Collecting data for 2003 page 0...\n",
      "Collecting data for 2004 page 0...\n",
      "Collecting data for 2005 page 0...\n",
      "Collecting data for 2006 page 0...\n",
      "Collecting data for 2007 page 0...\n",
      "Collecting data for 2008 page 0...\n",
      "Collecting data for 2009 page 0...\n",
      "Collecting data for 2010 page 0...\n",
      "Collecting data for 2011 page 0...\n",
      "Collecting data for 2012 page 0...\n",
      "Collecting data for 2013 page 0...\n",
      "Collecting data for 2014 page 0...\n",
      "Collecting data for 2015 page 0...\n",
      "Collecting data for 2016 page 0...\n",
      "Collecting data for 2017 page 0...\n",
      "Collecting data for 2018 page 0...\n",
      "Collecting data for 2019 page 0...\n",
      "Collecting data for 2020 page 0...\n",
      "Collecting data for 2021 page 0...\n",
      "Collecting data for 2022 page 0...\n"
     ]
    }
   ],
   "source": [
    "# Write a CSV file with this data\n",
    "for year in range(2000, 2023):\n",
    "  page = 0\n",
    "  print(f'Collecting data for {year} page {page}...')\n",
    "  \n",
    "  # Retry a page multiple times if necessary\n",
    "  while True:\n",
    "    data = metacritic_scraper(year, page)\n",
    "    \n",
    "    # Stop when we reach a page with zero rows\n",
    "    if len(data) == 0:\n",
    "      break\n",
    "    \n",
    "    # Convert the dataframe into a list of movies to insert into MongoDB\n",
    "    movies_to_insert = []\n",
    "\n",
    "    for row in data.itertuples():\n",
    "      movie = {\n",
    "        'title': row.title,\n",
    "        'release_date': row.date,\n",
    "        'description': row.description,\n",
    "        'metascore': row.score,\n",
    "        'image_url': row.image\n",
    "      }\n",
    "      movies_to_insert.append(movie)\n",
    "      \n",
    "    # Insert records into MongoDB\n",
    "    print(f'Inserting {len(movies_to_insert)} movies for the year {year} page {page}') \n",
    "    metacritic_data.insert_many(movies_to_insert)\n",
    "    page = page + 1\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
